{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DL.png\" width=\"400px\" height=\"400px\" >  \n",
    "\n",
    "\n",
    "**Team Members:**\n",
    "* Ahad Alsulami \n",
    "* Raneem Alomari\n",
    "* Bedoor Almareni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "0M2OGHiBtyMfy6XkEY0b6S",
     "type": "MD"
    }
   },
   "source": [
    "# About 🍲️\n",
    "\n",
    "**A recipe generator is a tool or software that uses algorithms to create unique recipes based on specific criteria such as ingredients, dietary restrictions, cooking methods, and cuisine preferences. These generators are designed to help individuals find new and interesting ways to prepare meals and provide inspiration for creating dishes they may not have considered before**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "DDCAAnTXGa7aLMn790Sbr4",
     "type": "MD"
    }
   },
   "source": [
    "## Importing  libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Using cached accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anilk\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\anilk\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from accelerate) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\anilk\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anilk\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\anilk\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Using cached accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.30.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Rt0t99f14VIEUHhFyxjO8W",
     "report_properties": {
      "rowId": "Ddu94u6qzc0tMocTOV2t4V"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "4a2IEWMpxL69gHBtrycBS3",
     "type": "MD"
    }
   },
   "source": [
    "## **Model configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Gw28g2UgVOsEjuU4ontL2Y",
     "report_properties": {
      "rowId": "UA5cBMlo662l8joHq6zi2D"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'gpt2'# Name of the pre-trained GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "4TpAVQr6YtElV2DhIuYfCG",
     "report_properties": {
      "rowId": "mwoENNvka6um4ZRoR61rsc"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "model_save_path = './DLProejectGPT' # Path to save the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "tmohMeDnmYgucw4jMwXOU5",
     "type": "MD"
    }
   },
   "source": [
    "## **Tokenizer initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "dKfCR80HNODcEgVEAFDBYD",
     "report_properties": {
      "rowId": "0e5nDha6BkgattyU6LnSHl"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anilk\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50260, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name,\n",
    "                                              bos_token='<|startoftext|>',# Beginning of sentence token\n",
    "                                              eos_token='<|endoftext|>',# End of sentence token\n",
    "                                              unk_token='<|unknown|>', # Unknown token\n",
    "                                              pad_token='<|pad|>'# Padding token\n",
    "                                             )\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)# Initialize the GPT2 model\n",
    "model.resize_token_embeddings(len(tokenizer))# Resize the token embeddings to match the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "uSmyzA3G8krLGqgYhE6fqa",
     "type": "MD"
    }
   },
   "source": [
    "**GPT2LMHeadModel is a pre-trained language model based on the GPT-2 architecture. It is designed to generate text by predicting the next word in a sequence given the previous words. It is called a \"language model\" because it models the probability distribution of words in a language.\n",
    "The \"LMHead\" in the name stands for \"Language Model Head\", which refers to the fact that the model is trained to predict the next word in a sequence. The \"Head\" part of the name is because this is the final layer of the model, which produces the output. In this specific code, the GPT2LMHeadModel is used to generate recipes by predicting the next word in the recipe based on the previous words.\n",
    "The Trainer is a class provided by the Hugging Face transformers library that is used to train and evaluate models. It provides an easy-to-use interface for training and fine-tuning models, including handling data loading, batching, and optimization.In this code, the Trainer is used to fine-tune the GPT2LMHeadModel on a custom recipe dataset. It takes care of training the model for a specified number of epochs, handling the batching of data, and applying the specified optimizer and learning rate scheduler.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "nAPyOlNeriLsBWTLs5lP5c",
     "report_properties": {
      "rowId": "eXKKZ3xa70ZakHgxyWHvpU"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./DLProejectGPT\\\\tokenizer_config.json',\n",
       " './DLProejectGPT\\\\special_tokens_map.json',\n",
       " './DLProejectGPT\\\\vocab.json',\n",
       " './DLProejectGPT\\\\merges.txt',\n",
       " './DLProejectGPT\\\\added_tokens.json',\n",
       " './DLProejectGPT\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(model_save_path)# Save the tokenizer to the specified model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "4c9FRjKZnKjpo2AjCmx8i1",
     "report_properties": {
      "rowId": "CErXK2yFPUKZP3GVpB2fjG"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50259]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['<|pad|>'])# Convert the empty token to its corresponding token ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "yQOmEQq5LQ2dFfYz3iXV9u",
     "type": "MD"
    }
   },
   "source": [
    "**This generate function takes a prompt as input, encodes it using the tokenizer, generates output text based on the prompt using the model, and finally decodes and prints the generated text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "8BC5pRp1jxjR0cUEek45Uc",
     "report_properties": {
      "rowId": "gJfhcggvZJMXpoq8dVa7nG"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    # Encode the prompt using the tokenizer\n",
    "    inputs = tokenizer.encode_plus(prompt, return_tensors='pt')\n",
    "    # Generate output text based on the prompt using the model\n",
    "    output = model.generate(**inputs,max_length=256,do_sample=True,pad_token_id=50259)\n",
    "    # Decode and print the generated text\n",
    "    print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Hpx7W2fGKYcSW9SNB3AQbH",
     "report_properties": {
      "rowId": "9WQc0pdI3a2ISJIvj7WaSe"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|startoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|unknown|>',\n",
       " 'pad_token': '<|pad|>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the special tokens map from the tokenizer\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "si2hOB35r5Vu85YucgYCIx",
     "report_properties": {
      "rowId": "gt0jp8cQL8rXtxqw6bTi7p"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50257]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['<|startoftext|>'],)# Convert the empty token to its corresponding token ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "M7HsbnsRDKwFSXl3a5UW3e",
     "type": "MD"
    }
   },
   "source": [
    "## **Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "0s8H37ECP8ErbIXyNstLj7",
     "report_properties": {
      "rowId": "TFJs9rPdjEhQtOjSJZkrJ8"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame called 'clean'\n",
    "clean = pd.read_csv('D:/Food_Recipe_Dataset.csv')\n",
    "# Shuffle the rows of the DataFrame\n",
    "clean = clean.sample(frac=1)\n",
    "# Reset the index of the DataFrame\n",
    "clean.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "MiV11DM8ADEZuqFz896ZFK",
     "report_properties": {
      "rowId": "GdCRIGhPC9yUQvSOW2paB6"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indian' 'Continental' 'South Indian Recipes' 'North Indian Recipes'\n",
      " 'Sindhi' 'Mangalorean' 'Malabar' 'Fusion' 'French' 'Italian Recipes'\n",
      " 'Mexican' 'Coorg' 'Kerala Recipes' 'Maharashtrian Recipes'\n",
      " 'Coastal Karnataka' 'Chettinad' 'Kashmiri' 'Rajasthani' 'Sri Lankan'\n",
      " 'Thai' 'Punjabi' 'Asian' 'Lucknowi' 'Tamil Nadu' 'Indo Chinese'\n",
      " 'Japanese' 'Karnataka' 'Middle Eastern' 'Goan Recipes' 'African'\n",
      " 'Parsi Recipes' 'Uttar Pradesh' 'Cantonese' 'Mediterranean'\n",
      " 'Gujarati Recipes\\ufeff' 'Awadhi' 'Bengali Recipes' 'Nepalese' 'Andhra'\n",
      " 'Jewish' 'Udupi' 'Himachal' 'Greek' 'Chinese' 'Caribbean' 'Oriya Recipes'\n",
      " 'North East India Recipes' 'Assamese' 'Konkan' 'Vietnamese' 'Malvani'\n",
      " 'Afghan' 'Arab' 'South Karnataka' 'Pakistani' 'Bihari' 'Mughlai'\n",
      " 'Indonesian' 'Hyderabadi' 'Haryana' 'European' 'Kongunadu' 'Malaysian'\n",
      " 'North Karnataka' 'American' 'World Breakfast' 'Burmese' 'Sichuan'\n",
      " 'Uttarakhand-North Kumaon' 'Korean' 'Dessert' 'Nagaland' 'Hunan'\n",
      " 'British' 'Side Dish' 'Snack' 'Dinner' 'Shandong' 'Appetizer' 'Lunch'\n",
      " 'Jharkhand' 'Brunch']\n"
     ]
    }
   ],
   "source": [
    "print(clean['Cuisine'].unique())#unique cuisine found in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "JSa2QSNCspdRP5SFgTJzD2",
     "report_properties": {
      "rowId": "PK7tYyUPIqM9m5y00CqQn8"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def print_recipe(idx):\n",
    "    # Print the ingredients and instructions of the recipe at the specified index.\n",
    "    print(f\"{clean['ingredients'][idx]}\\n\\n{clean['instructions'][idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "3tjYavlVB0vwzMXfk0HLv6",
     "type": "MD"
    }
   },
   "source": [
    "**the form_string function that takes an ingredient and an instruction as inputs and returns a formatted string combining them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Bo9cFGWDSrgwojp6haq42u",
     "report_properties": {
      "rowId": "uia0dwjWoYfHu12hiTqQ3i"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def form_string(ingredient,instruction):\n",
    "    # Formulate the string combining the ingredients and instructions\n",
    "    s = f\"<|startoftext|>Ingredients:\\n{ingredient.strip()}\\n\\nInstructions:\\n{instruction.strip()}<|endoftext|>\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "OVlWECAz2VwuW12BUR1xN7",
     "report_properties": {
      "rowId": "2dFxZsU0RcsYES7OG6gYin"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the form_string function to each row in the clean DataFrame\n",
    "# using 'TranslatedIngredients' and 'TranslatedInstructions' columns as inputs\n",
    "data = clean.apply(lambda x:form_string(x['TranslatedIngredients'],x['TranslatedInstructions']),axis=1).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "G4AipDPe16LXpBAtJRqEdX",
     "report_properties": {
      "rowId": "r4QuqTNHfMCSqDX5dvneeL"
     },
     "type": "MD"
    }
   },
   "source": [
    "https://towardsdatascience.com/guide-to-fine-tuning-text-generation-models-gpt-2-gpt-neo-and-t5-dc5de6b3bc5e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "NXW8ZGxhh8G94J1WVC5QDY",
     "type": "MD"
    }
   },
   "source": [
    "## **splits the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Zx9QHYJ09T811FHzQzM0A2",
     "report_properties": {
      "rowId": "mAlaxpQC3a1A8rVYVx0Cvq"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Set the proportion of data to be used for training\n",
    "train_size = 0.85\n",
    "# Calculate the length of the training set based on the specified train_size\n",
    "train_len = int(train_size * len(data))\n",
    "# Split the data into training and validation sets\n",
    "train_data = data[:train_len]# Contains the first train_len elements for training\n",
    "val_data = data[train_len:] # Contains the remaining elements for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "0LMJufRJqHbOr2hUIwgywa",
     "type": "MD"
    }
   },
   "source": [
    "**Defines a RecipeDataset class, which is a PyTorch dataset for working with recipe data. It takes a data list as input during initialization.**\n",
    "\n",
    "**The RecipeDataset class has three main methods:**\n",
    "\n",
    "1 - Initializes the dataset by processing the data list. It tokenizes and encodes each item in the data list using the tokenizer. The resulting input_ids and attention_masks are stored in separate lists self.input_ids and self.attn_masks.**\n",
    "\n",
    "2 - Returns the total number of items in the dataset, which is the length of the data list.\n",
    "\n",
    "3 - Returns the input_ids and attention_masks for the item at the given index idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "1ps3GeWplJLJaes9eGpjld",
     "report_properties": {
      "rowId": "lANwUfnsjpRhJe2HfQKJiz"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class RecipeDataset:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        # Iterate over the data and process each item\n",
    "        for item in tqdm(data):\n",
    "            # Tokenize and encode the item using the tokenizer\n",
    "            encodings = tokenizer.encode_plus(item,\n",
    "                                              truncation=True,\n",
    "                                              padding='max_length',\n",
    "                                              max_length=1024,\n",
    "                                              return_tensors='pt'\n",
    "                                             )\n",
    "            # Extract and store the input_ids and attention_masks\n",
    "            self.input_ids.append(torch.squeeze(encodings['input_ids'],0))\n",
    "            self.attn_masks.append(torch.squeeze(encodings['attention_mask'],0))\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the total number of items in the dataset\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # Return the input_ids and attention_masks for the item at the given index\n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "l0rKvNSkAMTK8vQeWWi3U2",
     "type": "MD"
    }
   },
   "source": [
    "**collate_fn function collate a batch of data samples in the custom RecipeDataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "HGnUeweCYmVMeVEE63do6H",
     "report_properties": {
      "rowId": "eCCLnfRmbtcC4UAr1mGoAB"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Stack the input_ids, attention_mask, and labels tensors in the batch\n",
    "    return {\n",
    "        'input_ids': torch.stack([item[0] for item in batch]),\n",
    "        'attention_mask': torch.stack([item[1] for item in batch]),\n",
    "        'labels': torch.stack([item[0] for item in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "0EqTkPFyiRJFcjKlI4Osml",
     "report_properties": {
      "rowId": "ZacWPRPonxY4DecLiVPBqD"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8ae4eda4bf4f789201adb73d511c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688f6010bd1e471780584e118b98abe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/891 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize train_ds with the training data\n",
    "train_ds = RecipeDataset(train_data)\n",
    "# Initialize val_ds with the validation data\n",
    "val_ds = RecipeDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "8qVuuIRmBqNJNAXvmV6Ygz",
     "report_properties": {
      "rowId": "Clbywyj8TYmhDKKnYzUeBl"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize args with the training arguments and settings\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_save_path,  # Directory to save the trained model\n",
    "    per_device_train_batch_size=2,  # Batch size for training on each device\n",
    "    per_device_eval_batch_size=2,  # Batch size for evaluation on each device\n",
    "    gradient_accumulation_steps=2,  # Number of steps to accumulate gradients before performing optimization\n",
    "    report_to='none',  # Disable reporting of training progress\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    save_strategy='no'  # Disable saving of checkpoints during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "me4BB8wpXJ5x6VDgLMh9qA",
     "report_properties": {
      "rowId": "bONIL9KJ0AZNBqP3UbnwKN"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the optimizer using the AdamW algorithm\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# Initialize the scheduler using the CosineAnnealingWarmRestarts method\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim, 20, eta_min=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "c0veGgKZQRpOiPTSUJ18SC",
     "report_properties": {
      "rowId": "6ApWhjBj1HZyZ4ouxPloYp"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the trainer object for model training\n",
    "trainer = Trainer(\n",
    "    model,  # The model to be trained\n",
    "    args,  # The training arguments and settings\n",
    "    train_dataset=train_ds,  # The training dataset\n",
    "    eval_dataset=val_ds,  # The validation dataset\n",
    "    data_collator=collate_fn,  # The collate function for batching the data\n",
    "    optimizers=(optim, scheduler)  # The optimizer and scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "sl23zKRmctgYMUya6FZc4B",
     "report_properties": {
      "rowId": "5ulOhjA5i9UoecXvi2Bjy3"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#starts the training process using the trainer object. \u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2209\u001b[0m ):\n\u001b[0;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3147\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:2125\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2125\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()#starts the training process using the trainer object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "FWnUq5jQCqwsyz2yYQUu7G",
     "report_properties": {
      "rowId": "mds9oDU6Imo0IjhH8M6PSD"
     },
     "type": "CODE"
    },
    "execution": {
     "iopub.execute_input": "2023-06-04T18:41:48.571414Z",
     "iopub.status.busy": "2023-06-04T18:41:48.570409Z",
     "iopub.status.idle": "2023-06-04T18:41:49.632405Z",
     "shell.execute_reply": "2023-06-04T18:41:49.631369Z",
     "shell.execute_reply.started": "2023-06-04T18:41:48.571372Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./DLProejectGPT\n",
      "Configuration saved in ./DLProejectGPT/config.json\n",
      "Model weights saved in ./DLProejectGPT/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()#to save the trained model after the training process is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "2fxQzIEegt0THyC0JBVPjJ",
     "report_properties": {
      "rowId": "yI6HCY6nfsrlB3SszNiJVC"
     },
     "type": "CODE"
    },
    "execution": {
     "iopub.execute_input": "2023-06-04T18:41:49.639273Z",
     "iopub.status.busy": "2023-06-04T18:41:49.636928Z",
     "iopub.status.idle": "2023-06-04T18:41:52.231128Z",
     "shell.execute_reply": "2023-06-04T18:41:52.230131Z",
     "shell.execute_reply.started": "2023-06-04T18:41:49.639236Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "rNRDH1h2zl3lBkzVF9qHnD",
     "report_properties": {
      "rowId": "0UWn53n7EfzvVJRhco5sIu"
     },
     "type": "CODE"
    },
    "execution": {
     "iopub.execute_input": "2023-06-04T18:41:52.233217Z",
     "iopub.status.busy": "2023-06-04T18:41:52.232530Z",
     "iopub.status.idle": "2023-06-04T18:41:54.725203Z",
     "shell.execute_reply": "2023-06-04T18:41:54.723860Z",
     "shell.execute_reply.started": "2023-06-04T18:41:52.233176Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /kaggle/working/DLProejectGPT/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"/kaggle/working/DLProejectGPT\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50260\n",
      "}\n",
      "\n",
      "loading configuration file /kaggle/working/DLProejectGPT/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"/kaggle/working/DLProejectGPT\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50260\n",
      "}\n",
      "\n",
      "loading weights file /kaggle/working/DLProejectGPT/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /kaggle/working/DLProejectGPT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading file /kaggle/working/DLProejectGPT/vocab.json\n",
      "loading file /kaggle/working/DLProejectGPT/merges.txt\n",
      "loading file /kaggle/working/DLProejectGPT/tokenizer.json\n",
      "loading file /kaggle/working/DLProejectGPT/added_tokens.json\n",
      "loading file /kaggle/working/DLProejectGPT/special_tokens_map.json\n",
      "loading file /kaggle/working/DLProejectGPT/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "pl = pipeline(task='text-generation',model='/kaggle/working/DLProejectGPT')#initializes a text generation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "FF0UhznC7N6of5yfJMQgKQ",
     "type": "MD"
    }
   },
   "source": [
    "**The create_prompt function takes a string of ingredients and a cuisine name as input and creates a formatted prompt string for generating a recipe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "bpjew3G5MeTh80cRt7BM7r",
     "report_properties": {
      "rowId": "Urkx8cHztgJEeXfDUK2eSW"
     },
     "type": "CODE"
    },
    "execution": {
     "iopub.execute_input": "2023-06-04T18:41:54.728058Z",
     "iopub.status.busy": "2023-06-04T18:41:54.727666Z",
     "iopub.status.idle": "2023-06-04T18:41:54.742311Z",
     "shell.execute_reply": "2023-06-04T18:41:54.739752Z",
     "shell.execute_reply.started": "2023-06-04T18:41:54.728013Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_prompt(cuisine,ingredients):\n",
    "    # Convert the ingredients to lowercase and remove leading/trailing whitespaces\n",
    "    ingredients = ','.join([x.strip().lower() for x in ingredients.split(',')])\n",
    "    # Replace commas with newline characters for better ingredient formatting\n",
    "    ingredients = ingredients.strip().replace(',', '\\n')\n",
    "    # Create the prompt string with the formatted ingredients and cuisine \n",
    "    s = f\"\\n\\nCuisine:\\n{Cuisine.value}\\n\\nIngredients:\\n{ingredients}\\n\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "zDoPMfLJx7rMb3rQcYVzyv",
     "report_properties": {
      "rowId": "31P9LK5YXIOPotImsWAuZX"
     },
     "type": "CODE"
    },
    "execution": {
     "iopub.execute_input": "2023-06-04T18:41:54.744816Z",
     "iopub.status.busy": "2023-06-04T18:41:54.744169Z",
     "iopub.status.idle": "2023-06-04T18:45:11.016512Z",
     "shell.execute_reply": "2023-06-04T18:45:11.015443Z",
     "shell.execute_reply.started": "2023-06-04T18:41:54.744776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Want to explore new flavors? Choose your cuisine preference!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495e763e796c48caa4f573a563b68d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('Sindhi', 'Mexican', 'Indian', 'Tamil Nadu', 'Chettinad', 'Goan Recipes', 'North Indian Reci…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Add your ingredients for a unique recipe!\n",
      "(separate them with a comma :)\n",
      " flour,sugar,cinnamon,vanilla\n"
     ]
    }
   ],
   "source": [
    "# Cusinie Selection\n",
    "print(\"Want to explore new flavors? Choose your cuisine preference!\\n\")\n",
    "Cuisine = widgets.Dropdown(options = clean['Cuisine'].unique(),\n",
    "                                value=None)\n",
    "display(Cuisine)\n",
    "\n",
    "\n",
    "# Ingredients Selection\n",
    "ingredients = [i for i in input(\"\\nAdd your ingredients for a unique recipe!\"+\n",
    "                                \"\\n(separate them with a comma :)\\n\").split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "18Fe7YXlxaquFRFyrLrQMC",
     "report_properties": {
      "rowId": "sFypHgKS21tXn9j5ptAC6h"
     },
     "type": "CODE"
    },
    "execution": {
     "iopub.execute_input": "2023-06-04T18:45:35.204251Z",
     "iopub.status.busy": "2023-06-04T18:45:35.203856Z",
     "iopub.status.idle": "2023-06-04T18:45:47.775010Z",
     "shell.execute_reply": "2023-06-04T18:45:47.773860Z",
     "shell.execute_reply.started": "2023-06-04T18:45:35.204218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cuisine:\n",
      "Arab\n",
      "\n",
      "Ingredients:\n",
      "flour\n",
      "sugar\n",
      "cinnamon\n",
      "vanilla\n",
      "\n",
      "1 cup raita flour, salt\n",
      "\n",
      "1 cup rice flour, 1/2 cup water, salt - as required\n",
      "\n",
      "pinch cinnamon, 1/2 cup sugar, 2 cups water, 1/2 teaspoon turmeric powder\n",
      "\n",
      "Instructions:\n",
      "To make the raita rice dough, firstly we will first make the raita.\n",
      "In a mixer, add the rice flour, salt, turmeric powder, cinnamon, sugar, water and grind to a smooth dough.\n",
      "Keep it aside.Now heat oil in a pan.\n",
      "Add cinnamon, sugar, turmeric and cook for 2 minutes.\n",
      "Once the spices start to sizzle, add the raita flour, rice flour, water and cook for 2 minutes.\n",
      "Add the remaining water and cook until the raita is cooked well.\n",
      "After 2 minutes, add the raita dough into the mixer and mix well.\n",
      "Check the salt and spice levels and adjust according to your taste.\n",
      "Serve the raita rice along with steamed rice and phulkas for a weekday meal.\n",
      "You can also serve it with phulkas for a wholesome lunch.\n"
     ]
    }
   ],
   "source": [
    "for ing in ingredients:\n",
    "    # Create a prompt using the current ingredient set and the specified cuisine\n",
    "    prompt = create_prompt(Cuisine.value,ing)\n",
    "    # Generate a recipe using the pipeline with specified parameters and print the generated recipe\n",
    "    print(pl(prompt,\n",
    "         max_new_tokens=512,\n",
    "         penalty_alpha=0.6,\n",
    "         top_k=4,\n",
    "         pad_token_id=50259\n",
    "        )[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
